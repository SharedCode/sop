package common

import (
	"context"
	"errors"
	"strings"
	"testing"
	"time"

	"github.com/sharedcode/sop"
	"github.com/sharedcode/sop/btree"
	"github.com/sharedcode/sop/cache"
	"github.com/sharedcode/sop/common/mocks"
)

// stubPriorityLogRemoveErr returns an error from Remove for a specific tid.
type stubPriorityLogRemoveErr struct {
	target string
}

func (s stubPriorityLogRemoveErr) IsEnabled() bool { return true }
func (s stubPriorityLogRemoveErr) Add(ctx context.Context, tid sop.UUID, payload []byte) error {
	return nil
}
func (s stubPriorityLogRemoveErr) Remove(ctx context.Context, tid sop.UUID) error {
	if tid.String() == s.target {
		return errors.New("prio remove err")
	}
	return nil
}
func (s stubPriorityLogRemoveErr) Get(ctx context.Context, tid sop.UUID) ([]sop.RegistryPayload[sop.Handle], error) {
	return nil, nil
}
func (s stubPriorityLogRemoveErr) GetBatch(ctx context.Context, batchSize int) ([]sop.KeyValuePair[sop.UUID, []sop.RegistryPayload[sop.Handle]], error) {
	return nil, nil
}
func (s stubPriorityLogRemoveErr) LogCommitChanges(ctx context.Context, stores []sop.StoreInfo, a, b, c, d []sop.RegistryPayload[sop.Handle]) error {
	return nil
}
func (s stubPriorityLogRemoveErr) ClearRegistrySectorClaims(ctx context.Context) error { return nil }

type tlogWithPrioRemoveErr struct{ pl stubPriorityLogRemoveErr }

func (t tlogWithPrioRemoveErr) PriorityLog() sop.TransactionPriorityLog { return t.pl }
func (t tlogWithPrioRemoveErr) Add(ctx context.Context, tid sop.UUID, commitFunction int, payload []byte) error {
	return nil
}
func (t tlogWithPrioRemoveErr) Remove(ctx context.Context, tid sop.UUID) error { return nil }
func (t tlogWithPrioRemoveErr) GetOne(ctx context.Context) (sop.UUID, string, []sop.KeyValuePair[int, []byte], error) {
	return sop.NilUUID, "", nil, nil
}
func (t tlogWithPrioRemoveErr) GetOneOfHour(ctx context.Context, hour string) (sop.UUID, []sop.KeyValuePair[int, []byte], error) {
	return sop.NilUUID, nil, nil
}
func (t tlogWithPrioRemoveErr) NewUUID() sop.UUID { return sop.NewUUID() }

// Ensures Transaction.rollback captures PriorityLog.Remove error when committedState >= beforeFinalize.
func Test_Transaction_Rollback_PriorityLogRemove_Error_ReturnsLastErr(t *testing.T) {
	ctx := context.Background()

	// Mocks and wiring
	l2 := mocks.NewMockClient()
	cache.NewGlobalCache(l2, cache.DefaultMinCapacity, cache.DefaultMaxCapacity)
	bs := mocks.NewMockBlobStore()
	reg := mocks.NewMockRegistry(false)
	sr := mocks.NewMockStoreRepository()

	tid := sop.NewUUID()
	tl := newTransactionLogger(tlogWithPrioRemoveErr{pl: stubPriorityLogRemoveErr{target: tid.String()}}, true)
	tl.committedState = beforeFinalize

	tx := &Transaction{id: tid, logger: tl, l2Cache: l2, l1Cache: cache.GetGlobalCache(), blobStore: bs, registry: reg, StoreRepository: sr}
	// Provide a nodeRepository so rollback() can call its methods safely even if no mutations.
	si := sop.NewStoreInfo(sop.StoreOptions{Name: "rb_prio_remove_err", SlotLength: 2})
	nr := &nodeRepositoryBackend{transaction: tx, storeInfo: si, l2Cache: l2, l1Cache: cache.GetGlobalCache(), localCache: map[sop.UUID]cachedNode{}, count: si.Count}
	tx.btreesBackend = []btreeBackend{{
		nodeRepository:                   nr,
		getStoreInfo:                     func() *sop.StoreInfo { return si },
		getForRollbackTrackedItemsValues: func() *sop.BlobsPayload[sop.UUID] { return nil },
		unlockTrackedItems:               func(context.Context) error { return nil },
	}}

	if err := tx.rollback(ctx, true); err == nil || err.Error() != "prio remove err" {
		t.Fatalf("expected 'prio remove err' from PriorityLog.Remove, got: %v", err)
	}
}

// addedErrOnceReg returns a sector-timeout sop.Error on first Add, then succeeds on subsequent calls.
type addedErrOnceReg struct {
	*mocks.Mock_vid_registry
	fired bool
	lk    sop.LockKey
}

func (r *addedErrOnceReg) Add(ctx context.Context, storesHandles []sop.RegistryPayload[sop.Handle]) error {
	if !r.fired {
		r.fired = true
		return sop.Error{Code: sop.RestoreRegistryFileSectorFailure, Err: errors.New("sector timeout"), UserData: &r.lk}
	}
	return r.Mock_vid_registry.Add(ctx, storesHandles)
}

// Exercise phase1Commit path where commitAddedNodes raises a sector-timeout once,
// handleRegistrySectorLockTimeout performs priority rollback, then retry succeeds.
func Test_Phase1Commit_CommitAddedNodes_SectorTimeout_Retry_Succeeds_Alt(t *testing.T) {
	ctx := context.Background()
	l2 := mocks.NewMockClient()
	cache.NewGlobalCache(l2, cache.DefaultMinCapacity, cache.DefaultMaxCapacity)
	bs := mocks.NewMockBlobStore()
	sr := mocks.NewMockStoreRepository()

	// Prepare registry that errors once on Add with a LockKey pointing to DTrollbk so takeover can succeed.
	base := mocks.NewMockRegistry(false).(*mocks.Mock_vid_registry)
	lk := sop.LockKey{Key: l2.FormatLockKey("DTrollbk"), LockID: sop.NewUUID()}
	reg := &addedErrOnceReg{Mock_vid_registry: base, lk: lk}

	// Transaction and logger that can handle priority rollback invoked by handleRegistrySectorLockTimeout.
	tl := newTransactionLogger(stubTLog{pl: &stubPriorityLog{}}, true)
	tx := &Transaction{mode: sop.ForWriting, maxTime: time.Minute, StoreRepository: sr, registry: reg, l2Cache: l2, l1Cache: cache.GetGlobalCache(), blobStore: bs, logger: tl, phaseDone: 0}

	// One added node to trigger commitAddedNodes.
	si := sop.NewStoreInfo(sop.StoreOptions{Name: "p1_added_sector_timeout", SlotLength: 2})
	n := &btree.Node[PersonKey, Person]{ID: sop.NewUUID(), Version: 0}
	nr := &nodeRepositoryBackend{transaction: tx, storeInfo: si, l2Cache: l2, l1Cache: cache.GetGlobalCache(), localCache: map[sop.UUID]cachedNode{n.ID: {action: addAction, node: n}}, count: si.Count}
	tx.btreesBackend = []btreeBackend{{
		nodeRepository:                   nr,
		getStoreInfo:                     func() *sop.StoreInfo { return si },
		hasTrackedItems:                  func() bool { return true },
		checkTrackedItems:                func(context.Context) error { return nil },
		lockTrackedItems:                 func(context.Context, time.Duration) error { return nil },
		unlockTrackedItems:               func(context.Context) error { return nil },
		commitTrackedItemsValues:         func(context.Context) error { return nil },
		getForRollbackTrackedItemsValues: func() *sop.BlobsPayload[sop.UUID] { return nil },
		getObsoleteTrackedItemsValues:    func() *sop.BlobsPayload[sop.UUID] { return nil },
		refetchAndMerge:                  func(context.Context) error { return nil },
	}}

	if err := tx.phase1Commit(ctx); err != nil {
		t.Fatalf("phase1Commit err: %v", err)
	}
}

// When committedState > finalizeCommit, rollback should immediately return a "can't rollback" error.
func Test_Transaction_Rollback_AfterCommit_ReturnsCommittedError(t *testing.T) {
	ctx := context.Background()

	l2 := mocks.NewMockClient()
	cache.NewGlobalCache(l2, cache.DefaultMinCapacity, cache.DefaultMaxCapacity)
	bs := mocks.NewMockBlobStore()
	reg := mocks.NewMockRegistry(false)
	sr := mocks.NewMockStoreRepository()

	tl := newTransactionLogger(stubTLog{pl: &stubPriorityLog{}}, true)
	// Simulate state past finalizeCommit (e.g., deleteObsoleteEntries)
	tl.committedState = deleteObsoleteEntries

	tx := &Transaction{id: sop.NewUUID(), logger: tl, l2Cache: l2, l1Cache: cache.GetGlobalCache(), blobStore: bs, registry: reg, StoreRepository: sr}
	// Minimal backend to avoid nil deref in rollback body if it were to proceed.
	si := sop.NewStoreInfo(sop.StoreOptions{Name: "rb_after_commit", SlotLength: 2})
	tx.btreesBackend = []btreeBackend{{
		nodeRepository:                   &nodeRepositoryBackend{transaction: tx, storeInfo: si, l2Cache: l2, l1Cache: cache.GetGlobalCache(), localCache: map[sop.UUID]cachedNode{}, count: si.Count},
		getStoreInfo:                     func() *sop.StoreInfo { return si },
		getForRollbackTrackedItemsValues: func() *sop.BlobsPayload[sop.UUID] { return nil },
		unlockTrackedItems:               func(context.Context) error { return nil },
	}}

	err := tx.rollback(ctx, true)
	if err == nil || err.Error() != "transaction got committed, 'can't rollback it" {
		t.Fatalf("expected committed error, got: %v", err)
	}
}

// If handleRegistrySectorLockTimeout receives a sector-timeout error with non-LockKey UserData,
// it should return the original error causing phase1Commit to fail fast.
type addedErrUserDataMismatch struct {
	*mocks.Mock_vid_registry
	fired bool
}

func (r *addedErrUserDataMismatch) Add(ctx context.Context, storesHandles []sop.RegistryPayload[sop.Handle]) error {
	if !r.fired {
		r.fired = true
		// UserData is intentionally not *sop.LockKey to exercise the mismatch path.
		return sop.Error{Code: sop.RestoreRegistryFileSectorFailure, Err: errors.New("sector timeout"), UserData: 1234}
	}
	return r.Mock_vid_registry.Add(ctx, storesHandles)
}

func Test_Phase1Commit_SectorTimeout_UserDataMismatch_ReturnsError(t *testing.T) {
	ctx := context.Background()

	l2 := mocks.NewMockClient()
	cache.NewGlobalCache(l2, cache.DefaultMinCapacity, cache.DefaultMaxCapacity)
	bs := mocks.NewMockBlobStore()
	sr := mocks.NewMockStoreRepository()

	base := mocks.NewMockRegistry(false).(*mocks.Mock_vid_registry)
	reg := &addedErrUserDataMismatch{Mock_vid_registry: base}

	tl := newTransactionLogger(stubTLog{pl: &stubPriorityLog{}}, true)
	tx := &Transaction{mode: sop.ForWriting, maxTime: time.Minute, StoreRepository: sr, registry: reg, l2Cache: l2, l1Cache: cache.GetGlobalCache(), blobStore: bs, logger: tl, phaseDone: 0}

	si := sop.NewStoreInfo(sop.StoreOptions{Name: "p1_added_userdata_mismatch", SlotLength: 2})
	n := &btree.Node[PersonKey, Person]{ID: sop.NewUUID(), Version: 0}
	nr := &nodeRepositoryBackend{transaction: tx, storeInfo: si, l2Cache: l2, l1Cache: cache.GetGlobalCache(), localCache: map[sop.UUID]cachedNode{n.ID: {action: addAction, node: n}}, count: si.Count}
	tx.btreesBackend = []btreeBackend{{
		nodeRepository:                   nr,
		getStoreInfo:                     func() *sop.StoreInfo { return si },
		hasTrackedItems:                  func() bool { return true },
		checkTrackedItems:                func(context.Context) error { return nil },
		lockTrackedItems:                 func(context.Context, time.Duration) error { return nil },
		unlockTrackedItems:               func(context.Context) error { return nil },
		commitTrackedItemsValues:         func(context.Context) error { return nil },
		getForRollbackTrackedItemsValues: func() *sop.BlobsPayload[sop.UUID] { return nil },
		getObsoleteTrackedItemsValues:    func() *sop.BlobsPayload[sop.UUID] { return nil },
		refetchAndMerge:                  func(context.Context) error { return nil },
	}}

	if err := tx.phase1Commit(ctx); err == nil {
		t.Fatalf("expected phase1Commit to fail due to UserData type mismatch on sector-timeout")
	}
}

// Covers the branch where commitTrackedItemsValues returns an error, causing phase1Commit to exit early.
func Test_Phase1Commit_CommitTrackedItemsValues_Error(t *testing.T) {
	ctx := context.Background()
	l2 := mocks.NewMockClient()
	cache.NewGlobalCache(l2, cache.DefaultMinCapacity, cache.DefaultMaxCapacity)
	bs := mocks.NewMockBlobStore()
	sr := mocks.NewMockStoreRepository()

	tl := newTransactionLogger(stubTLog{pl: &stubPriorityLog{}}, true)
	tx := &Transaction{mode: sop.ForWriting, maxTime: time.Minute, StoreRepository: sr, registry: mocks.NewMockRegistry(false), l2Cache: l2, l1Cache: cache.GetGlobalCache(), blobStore: bs, logger: tl, phaseDone: 0}

	si := sop.NewStoreInfo(sop.StoreOptions{Name: "p1_commit_values_err", SlotLength: 2})
	nr := &nodeRepositoryBackend{transaction: tx, storeInfo: si, l2Cache: l2, l1Cache: cache.GetGlobalCache(), localCache: map[sop.UUID]cachedNode{}, count: si.Count}
	want := errors.New("values commit error")
	tx.btreesBackend = []btreeBackend{{
		nodeRepository:                   nr,
		getStoreInfo:                     func() *sop.StoreInfo { return si },
		hasTrackedItems:                  func() bool { return true },
		checkTrackedItems:                func(context.Context) error { return nil },
		lockTrackedItems:                 func(context.Context, time.Duration) error { return nil },
		unlockTrackedItems:               func(context.Context) error { return nil },
		commitTrackedItemsValues:         func(context.Context) error { return want },
		getForRollbackTrackedItemsValues: func() *sop.BlobsPayload[sop.UUID] { return nil },
		getObsoleteTrackedItemsValues:    func() *sop.BlobsPayload[sop.UUID] { return nil },
		refetchAndMerge:                  func(context.Context) error { return nil },
	}}

	if err := tx.phase1Commit(ctx); err == nil || err.Error() != want.Error() {
		t.Fatalf("expected commit values error, got: %v", err)
	}
}

// Forces rollback to return an error (unlockTrackedItems fails) after a sector-timeout at commitAddedNodes,
// asserting phase1Commit wraps and returns the rollback error.
func Test_Phase1Commit_RollbackError_IsPropagated(t *testing.T) {
	ctx := context.Background()
	l2 := mocks.NewMockClient()
	cache.NewGlobalCache(l2, cache.DefaultMinCapacity, cache.DefaultMaxCapacity)
	bs := mocks.NewMockBlobStore()
	sr := mocks.NewMockStoreRepository()

	// Registry that triggers a sector-timeout once on Add with a valid LockKey for takeover
	base := mocks.NewMockRegistry(false).(*mocks.Mock_vid_registry)
	lk := sop.LockKey{Key: l2.FormatLockKey("DTrollbk"), LockID: sop.NewUUID()}
	reg := &addedErrOnceReg{Mock_vid_registry: base, lk: lk}

	tl := newTransactionLogger(stubTLog{pl: &stubPriorityLog{}}, true)
	tx := &Transaction{mode: sop.ForWriting, maxTime: time.Minute, StoreRepository: sr, registry: reg, l2Cache: l2, l1Cache: cache.GetGlobalCache(), blobStore: bs, logger: tl, phaseDone: 0}

	si := sop.NewStoreInfo(sop.StoreOptions{Name: "p1_rollback_err", SlotLength: 2})
	n := &btree.Node[PersonKey, Person]{ID: sop.NewUUID(), Version: 0}
	nr := &nodeRepositoryBackend{transaction: tx, storeInfo: si, l2Cache: l2, l1Cache: cache.GetGlobalCache(), localCache: map[sop.UUID]cachedNode{n.ID: {action: addAction, node: n}}, count: si.Count}
	tx.btreesBackend = []btreeBackend{{
		nodeRepository:                   nr,
		getStoreInfo:                     func() *sop.StoreInfo { return si },
		hasTrackedItems:                  func() bool { return true },
		checkTrackedItems:                func(context.Context) error { return nil },
		lockTrackedItems:                 func(context.Context, time.Duration) error { return nil },
		unlockTrackedItems:               func(context.Context) error { return errors.New("unlock tracked items fail") },
		commitTrackedItemsValues:         func(context.Context) error { return nil },
		getForRollbackTrackedItemsValues: func() *sop.BlobsPayload[sop.UUID] { return nil },
		getObsoleteTrackedItemsValues:    func() *sop.BlobsPayload[sop.UUID] { return nil },
		refetchAndMerge:                  func(context.Context) error { return nil },
	}}

	err := tx.phase1Commit(ctx)
	if err == nil || err.Error() != "phase 1 commit failed, then rollback errored with: unlock tracked items fail" {
		t.Fatalf("expected wrapped rollback error, got: %v", err)
	}
}

// Registry wrapper: UpdateNoLocks returns a plain error to exercise non-sop.Error path in commitUpdatedNodes.
type regUpdateNoLocksPlainErr struct{ *mocks.Mock_vid_registry }

func (r *regUpdateNoLocksPlainErr) UpdateNoLocks(ctx context.Context, allOrNothing bool, storesHandles []sop.RegistryPayload[sop.Handle]) error {
	return errors.New("upd nolocks plain err")
}

// Causes commitUpdatedNodes to return an error that is not sop.Error, so phase1Commit returns it immediately.
func Test_Phase1Commit_CommitUpdatedNodes_PlainError_Returns(t *testing.T) {
	ctx := context.Background()
	l2 := mocks.NewMockClient()
	cache.NewGlobalCache(l2, cache.DefaultMinCapacity, cache.DefaultMaxCapacity)
	bs := mocks.NewMockBlobStore()
	sr := mocks.NewMockStoreRepository()

	base := mocks.NewMockRegistry(false).(*mocks.Mock_vid_registry)
	reg := &regUpdateNoLocksPlainErr{Mock_vid_registry: base}

	tl := newTransactionLogger(stubTLog{pl: &stubPriorityLog{}}, true)
	tx := &Transaction{mode: sop.ForWriting, maxTime: time.Minute, StoreRepository: sr, registry: reg, l2Cache: l2, l1Cache: cache.GetGlobalCache(), blobStore: bs, logger: tl, phaseDone: 0}

	si := sop.NewStoreInfo(sop.StoreOptions{Name: "p1_upd_plain_err", SlotLength: 2})
	id := sop.NewUUID()
	node := &btree.Node[PersonKey, Person]{ID: id, Version: 1}
	nr := &nodeRepositoryBackend{transaction: tx, storeInfo: si, l2Cache: l2, l1Cache: cache.GetGlobalCache(), localCache: map[sop.UUID]cachedNode{id: {action: updateAction, node: node}}, count: si.Count}
	// Seed matching handle so commitUpdatedNodes proceeds until UpdateNoLocks
	h := sop.NewHandle(id)
	h.Version = 1
	base.Lookup[id] = h

	tx.btreesBackend = []btreeBackend{{
		nodeRepository:                   nr,
		getStoreInfo:                     func() *sop.StoreInfo { return si },
		hasTrackedItems:                  func() bool { return true },
		checkTrackedItems:                func(context.Context) error { return nil },
		lockTrackedItems:                 func(context.Context, time.Duration) error { return nil },
		unlockTrackedItems:               func(context.Context) error { return nil },
		commitTrackedItemsValues:         func(context.Context) error { return nil },
		getForRollbackTrackedItemsValues: func() *sop.BlobsPayload[sop.UUID] { return nil },
		getObsoleteTrackedItemsValues:    func() *sop.BlobsPayload[sop.UUID] { return nil },
		refetchAndMerge:                  func(context.Context) error { return nil },
	}}

	if err := tx.phase1Commit(ctx); err == nil || err.Error() != "upd nolocks plain err" {
		t.Fatalf("expected plain UpdateNoLocks error, got: %v", err)
	}
}

// Registry wrapper: Add returns a plain error (not sop.Error) to exercise direct error return in phase1Commit commitAddedNodes.
type regAddPlainErr struct{ *mocks.Mock_vid_registry }

func (r *regAddPlainErr) Add(ctx context.Context, storesHandles []sop.RegistryPayload[sop.Handle]) error {
	return errors.New("add plain err")
}

func Test_Phase1Commit_CommitAddedNodes_PlainError_Returns(t *testing.T) {
	ctx := context.Background()
	l2 := mocks.NewMockClient()
	cache.NewGlobalCache(l2, cache.DefaultMinCapacity, cache.DefaultMaxCapacity)
	bs := mocks.NewMockBlobStore()
	sr := mocks.NewMockStoreRepository()

	reg := &regAddPlainErr{Mock_vid_registry: mocks.NewMockRegistry(false).(*mocks.Mock_vid_registry)}
	tl := newTransactionLogger(stubTLog{pl: &stubPriorityLog{}}, true)
	tx := &Transaction{mode: sop.ForWriting, maxTime: time.Minute, StoreRepository: sr, registry: reg, l2Cache: l2, l1Cache: cache.GetGlobalCache(), blobStore: bs, logger: tl, phaseDone: 0}

	si := sop.NewStoreInfo(sop.StoreOptions{Name: "p1_add_plain_err", SlotLength: 2})
	n := &btree.Node[PersonKey, Person]{ID: sop.NewUUID(), Version: 0}
	nr := &nodeRepositoryBackend{transaction: tx, storeInfo: si, l2Cache: l2, l1Cache: cache.GetGlobalCache(), localCache: map[sop.UUID]cachedNode{n.ID: {action: addAction, node: n}}, count: si.Count}
	tx.btreesBackend = []btreeBackend{{
		nodeRepository:                   nr,
		getStoreInfo:                     func() *sop.StoreInfo { return si },
		hasTrackedItems:                  func() bool { return true },
		checkTrackedItems:                func(context.Context) error { return nil },
		lockTrackedItems:                 func(context.Context, time.Duration) error { return nil },
		unlockTrackedItems:               func(context.Context) error { return nil },
		commitTrackedItemsValues:         func(context.Context) error { return nil },
		getForRollbackTrackedItemsValues: func() *sop.BlobsPayload[sop.UUID] { return nil },
		getObsoleteTrackedItemsValues:    func() *sop.BlobsPayload[sop.UUID] { return nil },
		refetchAndMerge:                  func(context.Context) error { return nil },
	}}

	if err := tx.phase1Commit(ctx); err == nil || err.Error() != "add plain err" {
		t.Fatalf("expected plain Add error, got: %v", err)
	}
}

// Registry wrapper: Get returns an error to exercise commitRemovedNodes error propagation.
type regGetRemovedErr struct{ *mocks.Mock_vid_registry }

func (r *regGetRemovedErr) Get(ctx context.Context, storesLids []sop.RegistryPayload[sop.UUID]) ([]sop.RegistryPayload[sop.Handle], error) {
	return nil, errors.New("get removed err")
}

func Test_Phase1Commit_CommitRemovedNodes_GetError_Returns(t *testing.T) {
	ctx := context.Background()
	l2 := mocks.NewMockClient()
	cache.NewGlobalCache(l2, cache.DefaultMinCapacity, cache.DefaultMaxCapacity)
	bs := mocks.NewMockBlobStore()
	sr := mocks.NewMockStoreRepository()

	reg := &regGetRemovedErr{Mock_vid_registry: mocks.NewMockRegistry(false).(*mocks.Mock_vid_registry)}
	tl := newTransactionLogger(stubTLog{pl: &stubPriorityLog{}}, true)
	tx := &Transaction{mode: sop.ForWriting, maxTime: time.Minute, StoreRepository: sr, registry: reg, l2Cache: l2, l1Cache: cache.GetGlobalCache(), blobStore: bs, logger: tl, phaseDone: 0}

	si := sop.NewStoreInfo(sop.StoreOptions{Name: "p1_removed_get_err", SlotLength: 2})
	id := sop.NewUUID()
	n := &btree.Node[PersonKey, Person]{ID: id, Version: 1}
	nr := &nodeRepositoryBackend{transaction: tx, storeInfo: si, l2Cache: l2, l1Cache: cache.GetGlobalCache(), localCache: map[sop.UUID]cachedNode{id: {action: removeAction, node: n}}, count: si.Count}
	// Seed handle so convertToRegistryRequestPayload has a corresponding Lookup table (not necessary for Get error)
	tx.btreesBackend = []btreeBackend{{
		nodeRepository:                   nr,
		getStoreInfo:                     func() *sop.StoreInfo { return si },
		hasTrackedItems:                  func() bool { return true },
		checkTrackedItems:                func(context.Context) error { return nil },
		lockTrackedItems:                 func(context.Context, time.Duration) error { return nil },
		unlockTrackedItems:               func(context.Context) error { return nil },
		commitTrackedItemsValues:         func(context.Context) error { return nil },
		getForRollbackTrackedItemsValues: func() *sop.BlobsPayload[sop.UUID] { return nil },
		getObsoleteTrackedItemsValues:    func() *sop.BlobsPayload[sop.UUID] { return nil },
		refetchAndMerge:                  func(context.Context) error { return nil },
	}}

	if err := tx.phase1Commit(ctx); err == nil || err.Error() != "get removed err" {
		t.Fatalf("expected Get error in commitRemovedNodes, got: %v", err)
	}
}

// Ensures phase1Commit removes the pre-commit transaction logs when committedState indicates pre-commit work was recorded.
func Test_Phase1Commit_Removes_PreCommit_Log(t *testing.T) {
	ctx := context.Background()
	l2 := mocks.NewMockClient()
	cache.NewGlobalCache(l2, cache.DefaultMinCapacity, cache.DefaultMaxCapacity)
	bs := mocks.NewMockBlobStore()
	sr := mocks.NewMockStoreRepository()

	// Use tlRecorder from transactionlogger_scenarios_test.go to capture Remove calls.
	preTid := sop.NewUUID()
	rec := &tlRecorder{tid: preTid}
	tl := newTransactionLogger(rec, true)
	tl.committedState = addActivelyPersistedItem // simulate pre-commit stage

	tx := &Transaction{mode: sop.ForWriting, maxTime: time.Minute, StoreRepository: sr, registry: mocks.NewMockRegistry(false), l2Cache: l2, l1Cache: cache.GetGlobalCache(), blobStore: bs, logger: tl, phaseDone: 0}

	// Minimal backend: hasTrackedItems true, no nodes to mutate, and value commit succeeds.
	si := sop.NewStoreInfo(sop.StoreOptions{Name: "p1_precommit_remove", SlotLength: 2})
	nr := &nodeRepositoryBackend{transaction: tx, storeInfo: si, l2Cache: l2, l1Cache: cache.GetGlobalCache(), localCache: map[sop.UUID]cachedNode{}, count: si.Count}
	tx.btreesBackend = []btreeBackend{{
		nodeRepository:                   nr,
		getStoreInfo:                     func() *sop.StoreInfo { return si },
		hasTrackedItems:                  func() bool { return true },
		checkTrackedItems:                func(context.Context) error { return nil },
		lockTrackedItems:                 func(context.Context, time.Duration) error { return nil },
		unlockTrackedItems:               func(context.Context) error { return nil },
		commitTrackedItemsValues:         func(context.Context) error { return nil },
		getForRollbackTrackedItemsValues: func() *sop.BlobsPayload[sop.UUID] { return nil },
		getObsoleteTrackedItemsValues:    func() *sop.BlobsPayload[sop.UUID] { return nil },
		refetchAndMerge:                  func(context.Context) error { return nil },
	}}

	if err := tx.phase1Commit(ctx); err != nil {
		t.Fatalf("phase1Commit err: %v", err)
	}
	// Expect that the preTid was removed from logs by phase1Commit.
	if len(rec.removed) == 0 || rec.removed[0].Compare(preTid) != 0 {
		t.Fatalf("expected pre-commit log Remove(%s) to be called, got: %v", preTid.String(), rec.removed)
	}
}

// storeRepoUpdateErr wraps a base store repo and forces Update to return an error.
type storeRepoUpdateErr struct{ base sop.StoreRepository }

func (s storeRepoUpdateErr) Add(ctx context.Context, stores ...sop.StoreInfo) error {
	return s.base.Add(ctx, stores...)
}
func (s storeRepoUpdateErr) Update(ctx context.Context, stores []sop.StoreInfo) ([]sop.StoreInfo, error) {
	return nil, errors.New("store update err")
}
func (s storeRepoUpdateErr) Get(ctx context.Context, names ...string) ([]sop.StoreInfo, error) {
	return s.base.Get(ctx, names...)
}
func (s storeRepoUpdateErr) GetWithTTL(ctx context.Context, isCacheTTL bool, cacheDuration time.Duration, names ...string) ([]sop.StoreInfo, error) {
	return s.base.GetWithTTL(ctx, isCacheTTL, cacheDuration, names...)
}
func (s storeRepoUpdateErr) GetAll(ctx context.Context) ([]string, error) { return s.base.GetAll(ctx) }
func (s storeRepoUpdateErr) Remove(ctx context.Context, names ...string) error {
	return s.base.Remove(ctx, names...)
}
func (s storeRepoUpdateErr) Replicate(ctx context.Context, storesInfo []sop.StoreInfo) error {
	return s.base.Replicate(ctx, storesInfo)
}

// Validates that commitStoreInfo error is propagated by phase1Commit.
func Test_Phase1Commit_CommitStoreInfo_Error_Returns(t *testing.T) {
	ctx := context.Background()
	l2 := mocks.NewMockClient()
	cache.NewGlobalCache(l2, cache.DefaultMinCapacity, cache.DefaultMaxCapacity)
	bs := mocks.NewMockBlobStore()
	baseSR := mocks.NewMockStoreRepository()
	sr := storeRepoUpdateErr{base: baseSR}

	tl := newTransactionLogger(stubTLog{pl: &stubPriorityLog{}}, true)
	tx := &Transaction{mode: sop.ForWriting, maxTime: time.Minute, StoreRepository: sr, registry: mocks.NewMockRegistry(false), l2Cache: l2, l1Cache: cache.GetGlobalCache(), blobStore: bs, logger: tl, phaseDone: 0}

	// One added node so we go through the normal flow and reach commitStores.
	si := sop.NewStoreInfo(sop.StoreOptions{Name: "p1_commit_storeinfo_err", SlotLength: 2})
	n := &btree.Node[PersonKey, Person]{ID: sop.NewUUID(), Version: 0}
	nr := &nodeRepositoryBackend{transaction: tx, storeInfo: si, l2Cache: l2, l1Cache: cache.GetGlobalCache(), localCache: map[sop.UUID]cachedNode{n.ID: {action: addAction, node: n}}, count: si.Count}
	tx.btreesBackend = []btreeBackend{{
		nodeRepository:                   nr,
		getStoreInfo:                     func() *sop.StoreInfo { return si },
		hasTrackedItems:                  func() bool { return true },
		checkTrackedItems:                func(context.Context) error { return nil },
		lockTrackedItems:                 func(context.Context, time.Duration) error { return nil },
		unlockTrackedItems:               func(context.Context) error { return nil },
		commitTrackedItemsValues:         func(context.Context) error { return nil },
		getForRollbackTrackedItemsValues: func() *sop.BlobsPayload[sop.UUID] { return nil },
		getObsoleteTrackedItemsValues:    func() *sop.BlobsPayload[sop.UUID] { return nil },
		refetchAndMerge:                  func(context.Context) error { return nil },
	}}

	if err := tx.phase1Commit(ctx); err == nil || err.Error() != "store update err" {
		t.Fatalf("expected commit store info error, got: %v", err)
	}
}

// Non-empty root case: commitNewRootNodes returns (false,nil,nil), causing rollback and retry; after refetch,
// we switch classification so commit can succeed.
func Test_Phase1Commit_CommitNewRootNodes_NonEmptyRoot_Retry_Succeeds(t *testing.T) {
	ctx := context.Background()
	l2 := mocks.NewMockClient()
	cache.NewGlobalCache(l2, cache.DefaultMinCapacity, cache.DefaultMaxCapacity)
	bs := mocks.NewMockBlobStore()
	sr := mocks.NewMockStoreRepository()

	base := mocks.NewMockRegistry(false).(*mocks.Mock_vid_registry)
	tl := newTransactionLogger(stubTLog{pl: &stubPriorityLog{}}, true)
	tx := &Transaction{mode: sop.ForWriting, maxTime: time.Minute, StoreRepository: sr, registry: base, l2Cache: l2, l1Cache: cache.GetGlobalCache(), blobStore: bs, logger: tl, phaseDone: 0}

	// New store with pre-assigned root; classify as rootNodes when count==0 and action==addAction.
	si := sop.NewStoreInfo(sop.StoreOptions{Name: "p1_nonempty_root_retry", SlotLength: 2})
	// Ensure root id is set; use it for node ID.
	if si.RootNodeID.IsNil() {
		si.RootNodeID = sop.NewUUID()
	}
	node := &btree.Node[PersonKey, Person]{ID: si.RootNodeID, Version: 0}
	nr := &nodeRepositoryBackend{transaction: tx, storeInfo: si, l2Cache: l2, l1Cache: cache.GetGlobalCache(), localCache: map[sop.UUID]cachedNode{node.ID: {action: addAction, node: node}}, count: 0}

	// Seed registry to simulate non-empty root existing already so commitNewRootNodes returns false.
	h := sop.NewHandle(si.RootNodeID)
	base.Lookup[h.LogicalID] = h

	// After a failed attempt, refetch will adjust count so next classification treats it as a normal add and succeeds.
	refetchCalled := 0
	tx.btreesBackend = []btreeBackend{{
		nodeRepository:                   nr,
		getStoreInfo:                     func() *sop.StoreInfo { return si },
		hasTrackedItems:                  func() bool { return true },
		checkTrackedItems:                func(context.Context) error { return nil },
		lockTrackedItems:                 func(context.Context, time.Duration) error { return nil },
		unlockTrackedItems:               func(context.Context) error { return nil },
		commitTrackedItemsValues:         func(context.Context) error { return nil },
		getForRollbackTrackedItemsValues: func() *sop.BlobsPayload[sop.UUID] { return nil },
		getObsoleteTrackedItemsValues:    func() *sop.BlobsPayload[sop.UUID] { return nil },
		refetchAndMerge: func(context.Context) error {
			refetchCalled++
			nr.count = 1 // avoid root classification on retry
			return nil
		},
	}}

	if err := tx.phase1Commit(ctx); err != nil {
		t.Fatalf("phase1Commit err: %v", err)
	}
	if refetchCalled == 0 {
		t.Fatalf("expected refetch to be called due to non-empty root trigger")
	}
}

// When areFetchedItemsIntact returns false, phase1Commit rolls back and refetches, then retries and succeeds.
func Test_Phase1Commit_FetchedItems_NotIntact_Retry_Succeeds(t *testing.T) {
	ctx := context.Background()
	l2 := mocks.NewMockClient()
	cache.NewGlobalCache(l2, cache.DefaultMinCapacity, cache.DefaultMaxCapacity)
	bs := mocks.NewMockBlobStore()
	sr := mocks.NewMockStoreRepository()

	base := mocks.NewMockRegistry(false).(*mocks.Mock_vid_registry)
	tl := newTransactionLogger(stubTLog{pl: &stubPriorityLog{}}, true)
	tx := &Transaction{mode: sop.ForWriting, maxTime: time.Minute, StoreRepository: sr, registry: base, l2Cache: l2, l1Cache: cache.GetGlobalCache(), blobStore: bs, logger: tl, phaseDone: 0}

	si := sop.NewStoreInfo(sop.StoreOptions{Name: "p1_fetched_not_intact", SlotLength: 2})
	id := sop.NewUUID()
	node := &btree.Node[PersonKey, Person]{ID: id, Version: 1}
	// Seed registry with a newer version so areFetchedItemsIntact returns false first.
	h := sop.NewHandle(id)
	h.Version = 2
	base.Lookup[id] = h

	nr := &nodeRepositoryBackend{transaction: tx, storeInfo: si, l2Cache: l2, l1Cache: cache.GetGlobalCache(), localCache: map[sop.UUID]cachedNode{id: {action: getAction, node: node}}, count: si.Count}
	refetches := 0
	tx.btreesBackend = []btreeBackend{{
		nodeRepository:                   nr,
		getStoreInfo:                     func() *sop.StoreInfo { return si },
		hasTrackedItems:                  func() bool { return true },
		checkTrackedItems:                func(context.Context) error { return nil },
		lockTrackedItems:                 func(context.Context, time.Duration) error { return nil },
		unlockTrackedItems:               func(context.Context) error { return nil },
		commitTrackedItemsValues:         func(context.Context) error { return nil },
		getForRollbackTrackedItemsValues: func() *sop.BlobsPayload[sop.UUID] { return nil },
		getObsoleteTrackedItemsValues:    func() *sop.BlobsPayload[sop.UUID] { return nil },
		refetchAndMerge: func(context.Context) error {
			refetches++
			// Align local node version with registry so next check passes.
			node.Version = 2
			return nil
		},
	}}

	if err := tx.phase1Commit(ctx); err != nil {
		t.Fatalf("phase1Commit err: %v", err)
	}
	if refetches == 0 {
		t.Fatalf("expected a refetch due to fetched items not intact on first pass")
	}
}

// Note: isLockedFalseOnceCache is already defined in coverage_boost_19_test.go

// Covers branch: Lock ok but IsLocked returns false; phase1Commit should sleep/continue and then succeed on retry.
func Test_Phase1Commit_IsLockedFalse_Continue_Retry_Succeeds(t *testing.T) {
	ctx := context.Background()
	base := mocks.NewMockClient()
	l2 := &isLockedFalseOnceCache{Cache: base}
	cache.NewGlobalCache(l2, cache.DefaultMinCapacity, cache.DefaultMaxCapacity)
	bs := mocks.NewMockBlobStore()
	sr := mocks.NewMockStoreRepository()

	reg := mocks.NewMockRegistry(false).(*mocks.Mock_vid_registry)
	tl := newTransactionLogger(stubTLog{pl: &stubPriorityLog{}}, true)
	tx := &Transaction{mode: sop.ForWriting, maxTime: time.Minute, StoreRepository: sr, registry: reg, l2Cache: l2, l1Cache: cache.GetGlobalCache(), blobStore: bs, logger: tl, phaseDone: 0}

	si := sop.NewStoreInfo(sop.StoreOptions{Name: "p1_islocked_false_once", SlotLength: 2})
	id := sop.NewUUID()
	node := &btree.Node[PersonKey, Person]{ID: id, Version: 1}
	// Seed matching handle so commitUpdatedNodes proceeds
	h := sop.NewHandle(id)
	h.Version = 1
	reg.Lookup[id] = h
	nr := &nodeRepositoryBackend{transaction: tx, storeInfo: si, l2Cache: l2, l1Cache: cache.GetGlobalCache(), localCache: map[sop.UUID]cachedNode{id: {action: updateAction, node: node}}, count: si.Count}
	tx.btreesBackend = []btreeBackend{{
		nodeRepository:                   nr,
		getStoreInfo:                     func() *sop.StoreInfo { return si },
		hasTrackedItems:                  func() bool { return true },
		checkTrackedItems:                func(context.Context) error { return nil },
		lockTrackedItems:                 func(context.Context, time.Duration) error { return nil },
		unlockTrackedItems:               func(context.Context) error { return nil },
		commitTrackedItemsValues:         func(context.Context) error { return nil },
		getForRollbackTrackedItemsValues: func() *sop.BlobsPayload[sop.UUID] { return nil },
		getObsoleteTrackedItemsValues:    func() *sop.BlobsPayload[sop.UUID] { return nil },
		refetchAndMerge:                  func(context.Context) error { return nil },
	}}

	if err := tx.phase1Commit(ctx); err != nil {
		t.Fatalf("phase1Commit err: %v", err)
	}
}

// Note: lockFailOnceCache is already defined in coverage_boost_20_test.go

func Test_Phase1Commit_LockFailsOnce_RefetchAndMerge_Then_Succeeds(t *testing.T) {
	ctx := context.Background()
	base := mocks.NewMockClient()
	l2 := &lockFailOnceCache{Cache: base}
	cache.NewGlobalCache(l2, cache.DefaultMinCapacity, cache.DefaultMaxCapacity)
	bs := mocks.NewMockBlobStore()
	sr := mocks.NewMockStoreRepository()
	reg := mocks.NewMockRegistry(false).(*mocks.Mock_vid_registry)

	tl := newTransactionLogger(stubTLog{pl: &stubPriorityLog{}}, true)
	tx := &Transaction{mode: sop.ForWriting, maxTime: time.Minute, StoreRepository: sr, registry: reg, l2Cache: l2, l1Cache: cache.GetGlobalCache(), blobStore: bs, logger: tl, phaseDone: 0}

	si := sop.NewStoreInfo(sop.StoreOptions{Name: "p1_lock_fail_once", SlotLength: 2})
	id := sop.NewUUID()
	node := &btree.Node[PersonKey, Person]{ID: id, Version: 1}
	h := sop.NewHandle(id)
	h.Version = 1
	reg.Lookup[id] = h
	nr := &nodeRepositoryBackend{transaction: tx, storeInfo: si, l2Cache: l2, l1Cache: cache.GetGlobalCache(), localCache: map[sop.UUID]cachedNode{id: {action: updateAction, node: node}}, count: si.Count}
	refetchCalled := 0
	tx.btreesBackend = []btreeBackend{{
		nodeRepository:                   nr,
		getStoreInfo:                     func() *sop.StoreInfo { return si },
		hasTrackedItems:                  func() bool { return true },
		checkTrackedItems:                func(context.Context) error { return nil },
		lockTrackedItems:                 func(context.Context, time.Duration) error { return nil },
		unlockTrackedItems:               func(context.Context) error { return nil },
		commitTrackedItemsValues:         func(context.Context) error { return nil },
		getForRollbackTrackedItemsValues: func() *sop.BlobsPayload[sop.UUID] { return nil },
		getObsoleteTrackedItemsValues:    func() *sop.BlobsPayload[sop.UUID] { return nil },
		refetchAndMerge:                  func(context.Context) error { refetchCalled++; return nil },
	}}

	if err := tx.phase1Commit(ctx); err != nil {
		t.Fatalf("phase1Commit err: %v", err)
	}
	if refetchCalled == 0 {
		t.Fatalf("expected refetchAndMerge to be called due to initial lock failure")
	}
}

// TransactionLog stub that returns an error when logging a specific commitFunction.
type logErrTL struct{ target int }

func (l logErrTL) PriorityLog() sop.TransactionPriorityLog { return noOpPrioLog{} }
func (l logErrTL) Add(ctx context.Context, tid sop.UUID, commitFunction int, payload []byte) error {
	if commitFunction == l.target {
		return errors.New("log add err")
	}
	return nil
}
func (l logErrTL) Remove(ctx context.Context, tid sop.UUID) error { return nil }
func (l logErrTL) GetOne(ctx context.Context) (sop.UUID, string, []sop.KeyValuePair[int, []byte], error) {
	return sop.NilUUID, "", nil, nil
}
func (l logErrTL) GetOneOfHour(ctx context.Context, hour string) (sop.UUID, []sop.KeyValuePair[int, []byte], error) {
	return sop.NilUUID, nil, nil
}
func (l logErrTL) NewUUID() sop.UUID { return sop.NewUUID() }

// Hitting the log error at commitNewRootNodes.
func Test_Phase1Commit_LogCommitNewRootNodes_Error_Returns(t *testing.T) {
	ctx := context.Background()
	l2 := mocks.NewMockClient()
	cache.NewGlobalCache(l2, cache.DefaultMinCapacity, cache.DefaultMaxCapacity)
	bs := mocks.NewMockBlobStore()
	sr := mocks.NewMockStoreRepository()

	// logger that errors on commitNewRootNodes log
	tl := newTransactionLogger(logErrTL{target: int(commitNewRootNodes)}, true)
	reg := mocks.NewMockRegistry(false)
	tx := &Transaction{mode: sop.ForWriting, maxTime: time.Minute, StoreRepository: sr, registry: reg, l2Cache: l2, l1Cache: cache.GetGlobalCache(), blobStore: bs, logger: tl, phaseDone: 0}

	si := sop.NewStoreInfo(sop.StoreOptions{Name: "p1_log_newroot_err", SlotLength: 2})
	if si.RootNodeID.IsNil() {
		si.RootNodeID = sop.NewUUID()
	}
	n := &btree.Node[PersonKey, Person]{ID: si.RootNodeID, Version: 0}
	nr := &nodeRepositoryBackend{transaction: tx, storeInfo: si, l2Cache: l2, l1Cache: cache.GetGlobalCache(), localCache: map[sop.UUID]cachedNode{n.ID: {action: addAction, node: n}}, count: 0}
	tx.btreesBackend = []btreeBackend{{
		nodeRepository:                   nr,
		getStoreInfo:                     func() *sop.StoreInfo { return si },
		hasTrackedItems:                  func() bool { return true },
		checkTrackedItems:                func(context.Context) error { return nil },
		lockTrackedItems:                 func(context.Context, time.Duration) error { return nil },
		unlockTrackedItems:               func(context.Context) error { return nil },
		commitTrackedItemsValues:         func(context.Context) error { return nil },
		getForRollbackTrackedItemsValues: func() *sop.BlobsPayload[sop.UUID] { return nil },
		getObsoleteTrackedItemsValues:    func() *sop.BlobsPayload[sop.UUID] { return nil },
		refetchAndMerge:                  func(context.Context) error { return nil },
	}}

	if err := tx.phase1Commit(ctx); err == nil || err.Error() != "log add err" {
		t.Fatalf("expected log error at commitNewRootNodes, got: %v", err)
	}
}

// Hitting the log error at commitUpdatedNodes (after successful commitUpdatedNodes).
func Test_Phase1Commit_LogCommitUpdatedNodes_Error_Returns(t *testing.T) {
	ctx := context.Background()
	l2 := mocks.NewMockClient()
	cache.NewGlobalCache(l2, cache.DefaultMinCapacity, cache.DefaultMaxCapacity)
	bs := mocks.NewMockBlobStore()
	sr := mocks.NewMockStoreRepository()
	tl := newTransactionLogger(logErrTL{target: int(commitUpdatedNodes)}, true)
	baseReg := mocks.NewMockRegistry(false).(*mocks.Mock_vid_registry)
	tx := &Transaction{mode: sop.ForWriting, maxTime: time.Minute, StoreRepository: sr, registry: baseReg, l2Cache: l2, l1Cache: cache.GetGlobalCache(), blobStore: bs, logger: tl, phaseDone: 0}

	si := sop.NewStoreInfo(sop.StoreOptions{Name: "p1_log_upd_err", SlotLength: 2})
	id := sop.NewUUID()
	node := &btree.Node[PersonKey, Person]{ID: id, Version: 1}
	// Seed matching handle for success path through commitUpdatedNodes
	h := sop.NewHandle(id)
	h.Version = 1
	baseReg.Lookup[id] = h
	nr := &nodeRepositoryBackend{transaction: tx, storeInfo: si, l2Cache: l2, l1Cache: cache.GetGlobalCache(), localCache: map[sop.UUID]cachedNode{id: {action: updateAction, node: node}}, count: si.Count}
	tx.btreesBackend = []btreeBackend{{
		nodeRepository:                   nr,
		getStoreInfo:                     func() *sop.StoreInfo { return si },
		hasTrackedItems:                  func() bool { return true },
		checkTrackedItems:                func(context.Context) error { return nil },
		lockTrackedItems:                 func(context.Context, time.Duration) error { return nil },
		unlockTrackedItems:               func(context.Context) error { return nil },
		commitTrackedItemsValues:         func(context.Context) error { return nil },
		getForRollbackTrackedItemsValues: func() *sop.BlobsPayload[sop.UUID] { return nil },
		getObsoleteTrackedItemsValues:    func() *sop.BlobsPayload[sop.UUID] { return nil },
		refetchAndMerge:                  func(context.Context) error { return nil },
	}}

	if err := tx.phase1Commit(ctx); err == nil || err.Error() != "log add err" {
		t.Fatalf("expected log error at commitUpdatedNodes, got: %v", err)
	}
}

// Hitting the log error at areFetchedItemsIntact.
func Test_Phase1Commit_LogAreFetchedItemsIntact_Error_Returns(t *testing.T) {
	ctx := context.Background()
	l2 := mocks.NewMockClient()
	cache.NewGlobalCache(l2, cache.DefaultMinCapacity, cache.DefaultMaxCapacity)
	bs := mocks.NewMockBlobStore()
	sr := mocks.NewMockStoreRepository()

	tl := newTransactionLogger(logErrTL{target: int(areFetchedItemsIntact)}, true)
	reg := mocks.NewMockRegistry(false)
	tx := &Transaction{mode: sop.ForWriting, maxTime: time.Minute, StoreRepository: sr, registry: reg, l2Cache: l2, l1Cache: cache.GetGlobalCache(), blobStore: bs, logger: tl, phaseDone: 0}

	si := sop.NewStoreInfo(sop.StoreOptions{Name: "p1_log_fetchintact_err", SlotLength: 2})
	nr := &nodeRepositoryBackend{transaction: tx, storeInfo: si, l2Cache: l2, l1Cache: cache.GetGlobalCache(), localCache: map[sop.UUID]cachedNode{}, count: si.Count}
	tx.btreesBackend = []btreeBackend{{
		nodeRepository:                   nr,
		getStoreInfo:                     func() *sop.StoreInfo { return si },
		hasTrackedItems:                  func() bool { return true },
		checkTrackedItems:                func(context.Context) error { return nil },
		lockTrackedItems:                 func(context.Context, time.Duration) error { return nil },
		unlockTrackedItems:               func(context.Context) error { return nil },
		commitTrackedItemsValues:         func(context.Context) error { return nil },
		getForRollbackTrackedItemsValues: func() *sop.BlobsPayload[sop.UUID] { return nil },
		getObsoleteTrackedItemsValues:    func() *sop.BlobsPayload[sop.UUID] { return nil },
		refetchAndMerge:                  func(context.Context) error { return nil },
	}}

	if err := tx.phase1Commit(ctx); err == nil || err.Error() != "log add err" {
		t.Fatalf("expected log error at areFetchedItemsIntact, got: %v", err)
	}
}

// Hitting the log error at commitRemovedNodes.
func Test_Phase1Commit_LogCommitRemovedNodes_Error_Returns(t *testing.T) {
	ctx := context.Background()
	l2 := mocks.NewMockClient()
	cache.NewGlobalCache(l2, cache.DefaultMinCapacity, cache.DefaultMaxCapacity)
	bs := mocks.NewMockBlobStore()
	sr := mocks.NewMockStoreRepository()

	tl := newTransactionLogger(logErrTL{target: int(commitRemovedNodes)}, true)
	reg := mocks.NewMockRegistry(false)
	tx := &Transaction{mode: sop.ForWriting, maxTime: time.Minute, StoreRepository: sr, registry: reg, l2Cache: l2, l1Cache: cache.GetGlobalCache(), blobStore: bs, logger: tl, phaseDone: 0}

	si := sop.NewStoreInfo(sop.StoreOptions{Name: "p1_log_removed_err", SlotLength: 2})
	nr := &nodeRepositoryBackend{transaction: tx, storeInfo: si, l2Cache: l2, l1Cache: cache.GetGlobalCache(), localCache: map[sop.UUID]cachedNode{}, count: si.Count}
	tx.btreesBackend = []btreeBackend{{
		nodeRepository:                   nr,
		getStoreInfo:                     func() *sop.StoreInfo { return si },
		hasTrackedItems:                  func() bool { return true },
		checkTrackedItems:                func(context.Context) error { return nil },
		lockTrackedItems:                 func(context.Context, time.Duration) error { return nil },
		unlockTrackedItems:               func(context.Context) error { return nil },
		commitTrackedItemsValues:         func(context.Context) error { return nil },
		getForRollbackTrackedItemsValues: func() *sop.BlobsPayload[sop.UUID] { return nil },
		getObsoleteTrackedItemsValues:    func() *sop.BlobsPayload[sop.UUID] { return nil },
		refetchAndMerge:                  func(context.Context) error { return nil },
	}}

	if err := tx.phase1Commit(ctx); err == nil || err.Error() != "log add err" {
		t.Fatalf("expected log error at commitRemovedNodes, got: %v", err)
	}
}

// Hitting the log error at commitStoreInfo after phase1 loop completes.
func Test_Phase1Commit_LogCommitStoreInfo_Error_Returns(t *testing.T) {
	ctx := context.Background()
	l2 := mocks.NewMockClient()
	cache.NewGlobalCache(l2, cache.DefaultMinCapacity, cache.DefaultMaxCapacity)
	bs := mocks.NewMockBlobStore()
	sr := mocks.NewMockStoreRepository()

	tl := newTransactionLogger(logErrTL{target: int(commitStoreInfo)}, true)
	reg := mocks.NewMockRegistry(false)
	tx := &Transaction{mode: sop.ForWriting, maxTime: time.Minute, StoreRepository: sr, registry: reg, l2Cache: l2, l1Cache: cache.GetGlobalCache(), blobStore: bs, logger: tl, phaseDone: 0}

	si := sop.NewStoreInfo(sop.StoreOptions{Name: "p1_log_storeinfo_err", SlotLength: 2})
	nr := &nodeRepositoryBackend{transaction: tx, storeInfo: si, l2Cache: l2, l1Cache: cache.GetGlobalCache(), localCache: map[sop.UUID]cachedNode{}, count: si.Count}
	tx.btreesBackend = []btreeBackend{{
		nodeRepository:                   nr,
		getStoreInfo:                     func() *sop.StoreInfo { return si },
		hasTrackedItems:                  func() bool { return true },
		checkTrackedItems:                func(context.Context) error { return nil },
		lockTrackedItems:                 func(context.Context, time.Duration) error { return nil },
		unlockTrackedItems:               func(context.Context) error { return nil },
		commitTrackedItemsValues:         func(context.Context) error { return nil },
		getForRollbackTrackedItemsValues: func() *sop.BlobsPayload[sop.UUID] { return nil },
		getObsoleteTrackedItemsValues:    func() *sop.BlobsPayload[sop.UUID] { return nil },
		refetchAndMerge:                  func(context.Context) error { return nil },
	}}

	if err := tx.phase1Commit(ctx); err == nil || err.Error() != "log add err" {
		t.Fatalf("expected log error at commitStoreInfo, got: %v", err)
	}
}

// Covers rollbackRemovedNodes when nodesAreLocked=true and there are no flags to undo (no-op path).
func Test_NodeRepository_RollbackRemovedNodes_Locked_NoOp_Succeeds(t *testing.T) {
	ctx := context.Background()
	l2 := mocks.NewMockClient()
	cache.NewGlobalCache(l2, cache.DefaultMinCapacity, cache.DefaultMaxCapacity)
	bs := mocks.NewMockBlobStore()
	sr := mocks.NewMockStoreRepository()
	reg := mocks.NewMockRegistry(false).(*mocks.Mock_vid_registry)
	tl := newTransactionLogger(stubTLog{pl: &stubPriorityLog{}}, true)
	tx := &Transaction{mode: sop.ForWriting, maxTime: time.Minute, StoreRepository: sr, registry: reg, l2Cache: l2, l1Cache: cache.GetGlobalCache(), blobStore: bs, logger: tl, phaseDone: 0}

	si := sop.NewStoreInfo(sop.StoreOptions{Name: "rb_removed_locked_noop", SlotLength: 2})
	nr := &nodeRepositoryBackend{transaction: tx, storeInfo: si, l2Cache: l2, l1Cache: cache.GetGlobalCache(), localCache: map[sop.UUID]cachedNode{}, count: si.Count}
	id := sop.NewUUID()
	// Seed handle with no deleted/timestamp flags
	h := sop.NewHandle(id)
	reg.Lookup[id] = h
	vids := []sop.RegistryPayload[sop.UUID]{{RegistryTable: si.RegistryTable, IDs: []sop.UUID{id}}}
	if err := nr.rollbackRemovedNodes(ctx, true, vids); err != nil {
		t.Fatalf("rollbackRemovedNodes locked no-op err: %v", err)
	}
}

// Covers rollbackRemovedNodes when nodesAreLocked=true and flags exist to undo (UpdateNoLocks path).
func Test_NodeRepository_RollbackRemovedNodes_Locked_UpdateNoLocks_Succeeds(t *testing.T) {
	ctx := context.Background()
	l2 := mocks.NewMockClient()
	cache.NewGlobalCache(l2, cache.DefaultMinCapacity, cache.DefaultMaxCapacity)
	bs := mocks.NewMockBlobStore()
	sr := mocks.NewMockStoreRepository()
	reg := mocks.NewMockRegistry(false).(*mocks.Mock_vid_registry)
	tl := newTransactionLogger(stubTLog{pl: &stubPriorityLog{}}, true)
	tx := &Transaction{mode: sop.ForWriting, maxTime: time.Minute, StoreRepository: sr, registry: reg, l2Cache: l2, l1Cache: cache.GetGlobalCache(), blobStore: bs, logger: tl, phaseDone: 0}

	si := sop.NewStoreInfo(sop.StoreOptions{Name: "rb_removed_locked_upd", SlotLength: 2})
	nr := &nodeRepositoryBackend{transaction: tx, storeInfo: si, l2Cache: l2, l1Cache: cache.GetGlobalCache(), localCache: map[sop.UUID]cachedNode{}, count: si.Count}
	id := sop.NewUUID()
	// Seed handle with deleted flag and WIP timestamp to trigger rollback update.
	h := sop.NewHandle(id)
	h.IsDeleted = true
	h.WorkInProgressTimestamp = 123
	reg.Lookup[id] = h
	vids := []sop.RegistryPayload[sop.UUID]{{RegistryTable: si.RegistryTable, IDs: []sop.UUID{id}}}
	if err := nr.rollbackRemovedNodes(ctx, true, vids); err != nil {
		t.Fatalf("rollbackRemovedNodes locked update-nolocks err: %v", err)
	}
}

// Logger error on the initial lockTrackedItems log.
func Test_Phase1Commit_LogLockTrackedItems_Error_Returns(t *testing.T) {
	ctx := context.Background()
	l2 := mocks.NewMockClient()
	cache.NewGlobalCache(l2, cache.DefaultMinCapacity, cache.DefaultMaxCapacity)
	bs := mocks.NewMockBlobStore()
	sr := mocks.NewMockStoreRepository()
	tl := newTransactionLogger(logErrTL{target: int(lockTrackedItems)}, true)
	reg := mocks.NewMockRegistry(false)
	tx := &Transaction{mode: sop.ForWriting, maxTime: time.Minute, StoreRepository: sr, registry: reg, l2Cache: l2, l1Cache: cache.GetGlobalCache(), blobStore: bs, logger: tl, phaseDone: 0}
	si := sop.NewStoreInfo(sop.StoreOptions{Name: "p1_log_lock_items_err", SlotLength: 2})
	nr := &nodeRepositoryBackend{transaction: tx, storeInfo: si, l2Cache: l2, l1Cache: cache.GetGlobalCache(), localCache: map[sop.UUID]cachedNode{}, count: si.Count}
	tx.btreesBackend = []btreeBackend{{
		nodeRepository:                   nr,
		getStoreInfo:                     func() *sop.StoreInfo { return si },
		hasTrackedItems:                  func() bool { return true },
		checkTrackedItems:                func(context.Context) error { return nil },
		lockTrackedItems:                 func(context.Context, time.Duration) error { return nil },
		unlockTrackedItems:               func(context.Context) error { return nil },
		commitTrackedItemsValues:         func(context.Context) error { return nil },
		getForRollbackTrackedItemsValues: func() *sop.BlobsPayload[sop.UUID] { return nil },
		getObsoleteTrackedItemsValues:    func() *sop.BlobsPayload[sop.UUID] { return nil },
		refetchAndMerge:                  func(context.Context) error { return nil },
	}}
	if err := tx.phase1Commit(ctx); err == nil || err.Error() != "log add err" {
		t.Fatalf("expected log error at lockTrackedItems, got: %v", err)
	}
}

// Logger error on the commitTrackedItemsValues log.
func Test_Phase1Commit_LogCommitTrackedItemsValues_Error_Returns(t *testing.T) {
	ctx := context.Background()
	l2 := mocks.NewMockClient()
	cache.NewGlobalCache(l2, cache.DefaultMinCapacity, cache.DefaultMaxCapacity)
	bs := mocks.NewMockBlobStore()
	sr := mocks.NewMockStoreRepository()
	tl := newTransactionLogger(logErrTL{target: int(commitTrackedItemsValues)}, true)
	reg := mocks.NewMockRegistry(false)
	tx := &Transaction{mode: sop.ForWriting, maxTime: time.Minute, StoreRepository: sr, registry: reg, l2Cache: l2, l1Cache: cache.GetGlobalCache(), blobStore: bs, logger: tl, phaseDone: 0}
	si := sop.NewStoreInfo(sop.StoreOptions{Name: "p1_log_commit_values_err", SlotLength: 2})
	nr := &nodeRepositoryBackend{transaction: tx, storeInfo: si, l2Cache: l2, l1Cache: cache.GetGlobalCache(), localCache: map[sop.UUID]cachedNode{}, count: si.Count}
	tx.btreesBackend = []btreeBackend{{
		nodeRepository:                   nr,
		getStoreInfo:                     func() *sop.StoreInfo { return si },
		hasTrackedItems:                  func() bool { return true },
		checkTrackedItems:                func(context.Context) error { return nil },
		lockTrackedItems:                 func(context.Context, time.Duration) error { return nil },
		unlockTrackedItems:               func(context.Context) error { return nil },
		commitTrackedItemsValues:         func(context.Context) error { return nil },
		getForRollbackTrackedItemsValues: func() *sop.BlobsPayload[sop.UUID] { return nil },
		getObsoleteTrackedItemsValues:    func() *sop.BlobsPayload[sop.UUID] { return nil },
		refetchAndMerge:                  func(context.Context) error { return nil },
	}}
	if err := tx.phase1Commit(ctx); err == nil || err.Error() != "log add err" {
		t.Fatalf("expected log error at commitTrackedItemsValues, got: %v", err)
	}
}

// Logger error before commitAddedNodes.
func Test_Phase1Commit_LogCommitAddedNodes_Error_Returns(t *testing.T) {
	ctx := context.Background()
	l2 := mocks.NewMockClient()
	cache.NewGlobalCache(l2, cache.DefaultMinCapacity, cache.DefaultMaxCapacity)
	bs := mocks.NewMockBlobStore()
	sr := mocks.NewMockStoreRepository()
	tl := newTransactionLogger(logErrTL{target: int(commitAddedNodes)}, true)
	reg := mocks.NewMockRegistry(false)
	tx := &Transaction{mode: sop.ForWriting, maxTime: time.Minute, StoreRepository: sr, registry: reg, l2Cache: l2, l1Cache: cache.GetGlobalCache(), blobStore: bs, logger: tl, phaseDone: 0}
	si := sop.NewStoreInfo(sop.StoreOptions{Name: "p1_log_commit_added_err", SlotLength: 2})
	// Add one node so flow reaches commitAddedNodes
	n := &btree.Node[PersonKey, Person]{ID: sop.NewUUID(), Version: 0}
	nr := &nodeRepositoryBackend{transaction: tx, storeInfo: si, l2Cache: l2, l1Cache: cache.GetGlobalCache(), localCache: map[sop.UUID]cachedNode{n.ID: {action: addAction, node: n}}, count: si.Count}
	tx.btreesBackend = []btreeBackend{{
		nodeRepository:                   nr,
		getStoreInfo:                     func() *sop.StoreInfo { return si },
		hasTrackedItems:                  func() bool { return true },
		checkTrackedItems:                func(context.Context) error { return nil },
		lockTrackedItems:                 func(context.Context, time.Duration) error { return nil },
		unlockTrackedItems:               func(context.Context) error { return nil },
		commitTrackedItemsValues:         func(context.Context) error { return nil },
		getForRollbackTrackedItemsValues: func() *sop.BlobsPayload[sop.UUID] { return nil },
		getObsoleteTrackedItemsValues:    func() *sop.BlobsPayload[sop.UUID] { return nil },
		refetchAndMerge:                  func(context.Context) error { return nil },
	}}
	if err := tx.phase1Commit(ctx); err == nil || err.Error() != "log add err" {
		t.Fatalf("expected log error at commitAddedNodes, got: %v", err)
	}
}

// Logger error on beforeFinalize after commitStores and before final activation/touch.
func Test_Phase1Commit_LogBeforeFinalize_Error_Returns(t *testing.T) {
	ctx := context.Background()
	l2 := mocks.NewMockClient()
	cache.NewGlobalCache(l2, cache.DefaultMinCapacity, cache.DefaultMaxCapacity)
	bs := mocks.NewMockBlobStore()
	sr := mocks.NewMockStoreRepository()
	tl := newTransactionLogger(logErrTL{target: int(beforeFinalize)}, true)
	reg := mocks.NewMockRegistry(false)
	tx := &Transaction{mode: sop.ForWriting, maxTime: time.Minute, StoreRepository: sr, registry: reg, l2Cache: l2, l1Cache: cache.GetGlobalCache(), blobStore: bs, logger: tl, phaseDone: 0}
	si := sop.NewStoreInfo(sop.StoreOptions{Name: "p1_log_before_finalize_err", SlotLength: 2})
	nr := &nodeRepositoryBackend{transaction: tx, storeInfo: si, l2Cache: l2, l1Cache: cache.GetGlobalCache(), localCache: map[sop.UUID]cachedNode{}, count: si.Count}
	tx.btreesBackend = []btreeBackend{{
		nodeRepository:                   nr,
		getStoreInfo:                     func() *sop.StoreInfo { return si },
		hasTrackedItems:                  func() bool { return true },
		checkTrackedItems:                func(context.Context) error { return nil },
		lockTrackedItems:                 func(context.Context, time.Duration) error { return nil },
		unlockTrackedItems:               func(context.Context) error { return nil },
		commitTrackedItemsValues:         func(context.Context) error { return nil },
		getForRollbackTrackedItemsValues: func() *sop.BlobsPayload[sop.UUID] { return nil },
		getObsoleteTrackedItemsValues:    func() *sop.BlobsPayload[sop.UUID] { return nil },
		refetchAndMerge:                  func(context.Context) error { return nil },
	}}
	if err := tx.phase1Commit(ctx); err == nil || err.Error() != "log add err" {
		t.Fatalf("expected log error at beforeFinalize, got: %v", err)
	}
}

// When commitRemovedNodes detects a version conflict (returns successful=false), phase1Commit rolls back,
// refetches/merges, and retries; after aligning versions, it should succeed.
func Test_Phase1Commit_CommitRemovedNodes_Conflict_Retry_Succeeds_Alt(t *testing.T) {
	ctx := context.Background()
	l2 := mocks.NewMockClient()
	cache.NewGlobalCache(l2, cache.DefaultMinCapacity, cache.DefaultMaxCapacity)
	bs := mocks.NewMockBlobStore()
	sr := mocks.NewMockStoreRepository()
	base := mocks.NewMockRegistry(false).(*mocks.Mock_vid_registry)
	tl := newTransactionLogger(stubTLog{pl: &stubPriorityLog{}}, true)
	tx := &Transaction{mode: sop.ForWriting, maxTime: time.Minute, StoreRepository: sr, registry: base, l2Cache: l2, l1Cache: cache.GetGlobalCache(), blobStore: bs, logger: tl, phaseDone: 0}

	si := sop.NewStoreInfo(sop.StoreOptions{Name: "p1_removed_conflict_retry", SlotLength: 2})
	id := sop.NewUUID()
	node := &btree.Node[PersonKey, Person]{ID: id, Version: 1}
	// Seed registry with higher version so first commitRemovedNodes returns false
	h := sop.NewHandle(id)
	h.Version = 2
	base.Lookup[id] = h

	nr := &nodeRepositoryBackend{transaction: tx, storeInfo: si, l2Cache: l2, l1Cache: cache.GetGlobalCache(), localCache: map[sop.UUID]cachedNode{id: {action: removeAction, node: node}}, count: si.Count}
	retried := 0
	tx.btreesBackend = []btreeBackend{{
		nodeRepository:                   nr,
		getStoreInfo:                     func() *sop.StoreInfo { return si },
		hasTrackedItems:                  func() bool { return true },
		checkTrackedItems:                func(context.Context) error { return nil },
		lockTrackedItems:                 func(context.Context, time.Duration) error { return nil },
		unlockTrackedItems:               func(context.Context) error { return nil },
		commitTrackedItemsValues:         func(context.Context) error { return nil },
		getForRollbackTrackedItemsValues: func() *sop.BlobsPayload[sop.UUID] { return nil },
		getObsoleteTrackedItemsValues:    func() *sop.BlobsPayload[sop.UUID] { return nil },
		refetchAndMerge: func(context.Context) error {
			retried++
			node.Version = 2 // align version to let next commitRemovedNodes succeed
			return nil
		},
	}}

	if err := tx.phase1Commit(ctx); err != nil {
		t.Fatalf("phase1Commit err: %v", err)
	}
	if retried == 0 {
		t.Fatalf("expected refetch due to removed nodes version conflict")
	}
}

// Early return path: rollbackRemovedNodes with empty input should succeed trivially.
func Test_NodeRepository_RollbackRemovedNodes_EmptyInput(t *testing.T) {
	ctx := context.Background()
	l2 := mocks.NewMockClient()
	cache.NewGlobalCache(l2, cache.DefaultMinCapacity, cache.DefaultMaxCapacity)
	bs := mocks.NewMockBlobStore()
	sr := mocks.NewMockStoreRepository()
	reg := mocks.NewMockRegistry(false)
	tl := newTransactionLogger(stubTLog{pl: &stubPriorityLog{}}, true)
	tx := &Transaction{mode: sop.ForWriting, maxTime: time.Minute, StoreRepository: sr, registry: reg, l2Cache: l2, l1Cache: cache.GetGlobalCache(), blobStore: bs, logger: tl, phaseDone: 0}
	si := sop.NewStoreInfo(sop.StoreOptions{Name: "rb_removed_empty", SlotLength: 2})
	nr := &nodeRepositoryBackend{transaction: tx, storeInfo: si, l2Cache: l2, l1Cache: cache.GetGlobalCache(), localCache: map[sop.UUID]cachedNode{}, count: si.Count}
	if err := nr.rollbackRemovedNodes(ctx, true, nil); err != nil {
		t.Fatalf("rollbackRemovedNodes empty input err: %v", err)
	}
}

// Unlocked path with flags present to ensure Update() branch is exercised.
func Test_NodeRepository_RollbackRemovedNodes_Unlocked_Update_Succeeds(t *testing.T) {
	ctx := context.Background()
	l2 := mocks.NewMockClient()
	cache.NewGlobalCache(l2, cache.DefaultMinCapacity, cache.DefaultMaxCapacity)
	bs := mocks.NewMockBlobStore()
	sr := mocks.NewMockStoreRepository()
	reg := mocks.NewMockRegistry(false).(*mocks.Mock_vid_registry)
	tl := newTransactionLogger(stubTLog{pl: &stubPriorityLog{}}, true)
	tx := &Transaction{mode: sop.ForWriting, maxTime: time.Minute, StoreRepository: sr, registry: reg, l2Cache: l2, l1Cache: cache.GetGlobalCache(), blobStore: bs, logger: tl, phaseDone: 0}
	si := sop.NewStoreInfo(sop.StoreOptions{Name: "rb_removed_unlocked_upd", SlotLength: 2})
	nr := &nodeRepositoryBackend{transaction: tx, storeInfo: si, l2Cache: l2, l1Cache: cache.GetGlobalCache(), localCache: map[sop.UUID]cachedNode{}, count: si.Count}
	id := sop.NewUUID()
	// Seed handle flagged deleted & WIP so unlocked Update() path will be used
	h := sop.NewHandle(id)
	h.IsDeleted = true
	h.WorkInProgressTimestamp = 456
	reg.Lookup[id] = h
	vids := []sop.RegistryPayload[sop.UUID]{{RegistryTable: si.RegistryTable, IDs: []sop.UUID{id}}}
	if err := nr.rollbackRemovedNodes(ctx, false, vids); err != nil {
		t.Fatalf("rollbackRemovedNodes unlocked update err: %v", err)
	}
}

// Registry wrapper that errors on UpdateNoLocks to exercise locked error branch in rollbackRemovedNodes.
type errUpdateNoLocksReg2 struct{ inner sop.Registry }

func (e errUpdateNoLocksReg2) Add(ctx context.Context, s []sop.RegistryPayload[sop.Handle]) error {
	return e.inner.Add(ctx, s)
}
func (e errUpdateNoLocksReg2) Update(ctx context.Context, s []sop.RegistryPayload[sop.Handle]) error {
	return e.inner.Update(ctx, s)
}
func (e errUpdateNoLocksReg2) UpdateNoLocks(ctx context.Context, allOrNothing bool, s []sop.RegistryPayload[sop.Handle]) error {
	return errors.New("updateNoLocks err")
}
func (e errUpdateNoLocksReg2) Get(ctx context.Context, lids []sop.RegistryPayload[sop.UUID]) ([]sop.RegistryPayload[sop.Handle], error) {
	return e.inner.Get(ctx, lids)
}
func (e errUpdateNoLocksReg2) Remove(ctx context.Context, lids []sop.RegistryPayload[sop.UUID]) error {
	return e.inner.Remove(ctx, lids)
}
func (e errUpdateNoLocksReg2) Unlock(ctx context.Context, lk *sop.LockKey) error {
	return e.inner.Unlock(ctx, lk)
}
func (e errUpdateNoLocksReg2) Replicate(ctx context.Context, a, b, c, d []sop.RegistryPayload[sop.Handle]) error {
	return e.inner.Replicate(ctx, a, b, c, d)
}

// Covers rollbackRemovedNodes when nodesAreLocked=true and UpdateNoLocks returns an error.
func Test_NodeRepository_RollbackRemovedNodes_Locked_UpdateNoLocksError(t *testing.T) {
	ctx := context.Background()
	l2 := mocks.NewMockClient()
	cache.NewGlobalCache(l2, cache.DefaultMinCapacity, cache.DefaultMaxCapacity)
	bs := mocks.NewMockBlobStore()
	sr := mocks.NewMockStoreRepository()
	base := mocks.NewMockRegistry(false).(*mocks.Mock_vid_registry)
	reg := errUpdateNoLocksReg2{inner: base}
	tl := newTransactionLogger(stubTLog{pl: &stubPriorityLog{}}, true)
	tx := &Transaction{mode: sop.ForWriting, maxTime: time.Minute, StoreRepository: sr, registry: reg, l2Cache: l2, l1Cache: cache.GetGlobalCache(), blobStore: bs, logger: tl, phaseDone: 0}

	si := sop.NewStoreInfo(sop.StoreOptions{Name: "rb_removed_locked_upderr", SlotLength: 2})
	nr := &nodeRepositoryBackend{transaction: tx, storeInfo: si, l2Cache: l2, l1Cache: cache.GetGlobalCache(), localCache: map[sop.UUID]cachedNode{}, count: si.Count}
	id := sop.NewUUID()
	h := sop.NewHandle(id)
	h.IsDeleted = true
	h.WorkInProgressTimestamp = 99
	base.Lookup[id] = h
	vids := []sop.RegistryPayload[sop.UUID]{{RegistryTable: si.RegistryTable, IDs: []sop.UUID{id}}}
	if err := nr.rollbackRemovedNodes(ctx, true, vids); err == nil || err.Error() != "unable to undo removed nodes in registry, [{  ["+si.RegistryTable+"] ["+h.LogicalID.String()+"]}], error: updateNoLocks err" {
		// We don't assert exact formatted value structure to avoid coupling; just ensure error surfaces.
		if err == nil {
			t.Fatalf("expected error from UpdateNoLocks in locked path")
		}
	}
}

// Covers branch: IsLocked returns an error first; loop continues and retry succeeds.
func Test_Phase1Commit_IsLockedError_Continue_Retry_Succeeds(t *testing.T) {
	ctx := context.Background()
	base := mocks.NewMockClient()
	l2 := &isLockedErrOnceCache{Cache: base}
	cache.NewGlobalCache(l2, cache.DefaultMinCapacity, cache.DefaultMaxCapacity)
	bs := mocks.NewMockBlobStore()
	sr := mocks.NewMockStoreRepository()

	reg := mocks.NewMockRegistry(false).(*mocks.Mock_vid_registry)
	tl := newTransactionLogger(stubTLog{pl: &stubPriorityLog{}}, true)
	tx := &Transaction{mode: sop.ForWriting, maxTime: time.Minute, StoreRepository: sr, registry: reg, l2Cache: l2, l1Cache: cache.GetGlobalCache(), blobStore: bs, logger: tl, phaseDone: 0}

	si := sop.NewStoreInfo(sop.StoreOptions{Name: "p1_islocked_err_once", SlotLength: 2})
	id := sop.NewUUID()
	node := &btree.Node[PersonKey, Person]{ID: id, Version: 1}
	h := sop.NewHandle(id)
	h.Version = 1
	reg.Lookup[id] = h
	nr := &nodeRepositoryBackend{transaction: tx, storeInfo: si, l2Cache: l2, l1Cache: cache.GetGlobalCache(), localCache: map[sop.UUID]cachedNode{id: {action: updateAction, node: node}}, count: si.Count}
	tx.btreesBackend = []btreeBackend{{
		nodeRepository:                   nr,
		getStoreInfo:                     func() *sop.StoreInfo { return si },
		hasTrackedItems:                  func() bool { return true },
		checkTrackedItems:                func(context.Context) error { return nil },
		lockTrackedItems:                 func(context.Context, time.Duration) error { return nil },
		unlockTrackedItems:               func(context.Context) error { return nil },
		commitTrackedItemsValues:         func(context.Context) error { return nil },
		getForRollbackTrackedItemsValues: func() *sop.BlobsPayload[sop.UUID] { return nil },
		getObsoleteTrackedItemsValues:    func() *sop.BlobsPayload[sop.UUID] { return nil },
		refetchAndMerge:                  func(context.Context) error { return nil },
	}}

	if err := tx.phase1Commit(ctx); err != nil {
		t.Fatalf("phase1Commit err: %v", err)
	}
}

// Cache wrapper that forces Lock to fail for the DTrollbk takeover key used by handleRegistrySectorLockTimeout.
type dtLockFailCache struct{ sop.Cache }

func (c dtLockFailCache) Lock(ctx context.Context, duration time.Duration, lockKeys []*sop.LockKey) (bool, sop.UUID, error) {
	for _, k := range lockKeys {
		if strings.Contains(k.Key, "DTrollbk") { // takeover lock key
			return false, sop.NilUUID, nil
		}
	}
	return c.Cache.Lock(ctx, duration, lockKeys)
}

// Sector-timeout with correct UserData but DTrollbk takeover lock fails; expect original error returned.
func Test_Phase1Commit_SectorTimeout_TakeoverLock_Fails_ReturnsError(t *testing.T) {
	ctx := context.Background()
	base := mocks.NewMockClient()
	l2 := dtLockFailCache{Cache: base}
	cache.NewGlobalCache(l2, cache.DefaultMinCapacity, cache.DefaultMaxCapacity)
	bs := mocks.NewMockBlobStore()
	sr := mocks.NewMockStoreRepository()

	baseReg := mocks.NewMockRegistry(false).(*mocks.Mock_vid_registry)
	lk := sop.LockKey{Key: l2.FormatLockKey("DTrollbk"), LockID: sop.NewUUID()}
	reg := &addedErrOnceReg{Mock_vid_registry: baseReg, lk: lk}

	tl := newTransactionLogger(stubTLog{pl: &stubPriorityLog{}}, true)
	tx := &Transaction{mode: sop.ForWriting, maxTime: time.Minute, StoreRepository: sr, registry: reg, l2Cache: l2, l1Cache: cache.GetGlobalCache(), blobStore: bs, logger: tl, phaseDone: 0}

	si := sop.NewStoreInfo(sop.StoreOptions{Name: "p1_dt_takeover_lock_fail", SlotLength: 2})
	n := &btree.Node[PersonKey, Person]{ID: sop.NewUUID(), Version: 0}
	nr := &nodeRepositoryBackend{transaction: tx, storeInfo: si, l2Cache: l2, l1Cache: cache.GetGlobalCache(), localCache: map[sop.UUID]cachedNode{n.ID: {action: addAction, node: n}}, count: si.Count}
	tx.btreesBackend = []btreeBackend{{
		nodeRepository:                   nr,
		getStoreInfo:                     func() *sop.StoreInfo { return si },
		hasTrackedItems:                  func() bool { return true },
		checkTrackedItems:                func(context.Context) error { return nil },
		lockTrackedItems:                 func(context.Context, time.Duration) error { return nil },
		unlockTrackedItems:               func(context.Context) error { return nil },
		commitTrackedItemsValues:         func(context.Context) error { return nil },
		getForRollbackTrackedItemsValues: func() *sop.BlobsPayload[sop.UUID] { return nil },
		getObsoleteTrackedItemsValues:    func() *sop.BlobsPayload[sop.UUID] { return nil },
		refetchAndMerge:                  func(context.Context) error { return nil },
	}}

	if err := tx.phase1Commit(ctx); err == nil {
		t.Fatalf("expected phase1Commit to return original sector-timeout error when takeover lock fails")
	}
}

// Timed out early in phase1Commit before any work loop.
func Test_Phase1Commit_TimedOut_ReturnsError(t *testing.T) {
	ctx := context.Background()
	l2 := mocks.NewMockClient()
	cache.NewGlobalCache(l2, cache.DefaultMinCapacity, cache.DefaultMaxCapacity)
	bs := mocks.NewMockBlobStore()
	sr := mocks.NewMockStoreRepository()
	tl := newTransactionLogger(stubTLog{pl: &stubPriorityLog{}}, true)
	// Extremely small maxTime to trigger immediate timeout.
	tx := &Transaction{mode: sop.ForWriting, maxTime: time.Nanosecond, StoreRepository: sr, registry: mocks.NewMockRegistry(false), l2Cache: l2, l1Cache: cache.GetGlobalCache(), blobStore: bs, logger: tl, phaseDone: 0}
	si := sop.NewStoreInfo(sop.StoreOptions{Name: "p1_timeout", SlotLength: 2})
	nr := &nodeRepositoryBackend{transaction: tx, storeInfo: si, l2Cache: l2, l1Cache: cache.GetGlobalCache(), localCache: map[sop.UUID]cachedNode{}, count: si.Count}
	tx.btreesBackend = []btreeBackend{{
		nodeRepository:                   nr,
		getStoreInfo:                     func() *sop.StoreInfo { return si },
		hasTrackedItems:                  func() bool { return true },
		checkTrackedItems:                func(context.Context) error { return nil },
		lockTrackedItems:                 func(context.Context, time.Duration) error { return nil },
		unlockTrackedItems:               func(context.Context) error { return nil },
		commitTrackedItemsValues:         func(context.Context) error { return nil },
		getForRollbackTrackedItemsValues: func() *sop.BlobsPayload[sop.UUID] { return nil },
		getObsoleteTrackedItemsValues:    func() *sop.BlobsPayload[sop.UUID] { return nil },
		refetchAndMerge:                  func(context.Context) error { return nil },
	}}
	if err := tx.phase1Commit(ctx); err == nil {
		t.Fatalf("expected timeout error from phase1Commit")
	}
}

// TransactionLog stub that errors on the Nth occurrence of a specific commitFunction.
type logErrCounterTL struct{ target, fireOn, count int }

func (l *logErrCounterTL) PriorityLog() sop.TransactionPriorityLog { return noOpPrioLog{} }
func (l *logErrCounterTL) Add(ctx context.Context, tid sop.UUID, commitFunction int, payload []byte) error {
	if commitFunction == l.target {
		l.count++
		if l.count == l.fireOn {
			return errors.New("log add err nth")
		}
	}
	return nil
}
func (l *logErrCounterTL) Remove(ctx context.Context, tid sop.UUID) error { return nil }
func (l *logErrCounterTL) GetOne(ctx context.Context) (sop.UUID, string, []sop.KeyValuePair[int, []byte], error) {
	return sop.NilUUID, "", nil, nil
}
func (l *logErrCounterTL) GetOneOfHour(ctx context.Context, hour string) (sop.UUID, []sop.KeyValuePair[int, []byte], error) {
	return sop.NilUUID, nil, nil
}
func (l *logErrCounterTL) NewUUID() sop.UUID { return sop.NewUUID() }

// Refetch path: initial Lock of nodesKeys fails, needsRefetchAndMerge=true; on retry, logger.log(lockTrackedItems) inside refetch path fails.
func Test_Phase1Commit_Refetch_LogLockTrackedItems_Error_Returns(t *testing.T) {
	ctx := context.Background()
	base := mocks.NewMockClient()
	l2 := &lockFailOnceCache{Cache: base}
	cache.NewGlobalCache(l2, cache.DefaultMinCapacity, cache.DefaultMaxCapacity)
	bs := mocks.NewMockBlobStore()
	sr := mocks.NewMockStoreRepository()
	// Error on the 2nd lockTrackedItems log (the one inside refetch path)
	tl := newTransactionLogger(&logErrCounterTL{target: int(lockTrackedItems), fireOn: 2}, true)
	reg := mocks.NewMockRegistry(false).(*mocks.Mock_vid_registry)
	tx := &Transaction{mode: sop.ForWriting, maxTime: time.Minute, StoreRepository: sr, registry: reg, l2Cache: l2, l1Cache: cache.GetGlobalCache(), blobStore: bs, logger: tl, phaseDone: 0}
	si := sop.NewStoreInfo(sop.StoreOptions{Name: "p1_refetch_log_lock_items_err", SlotLength: 2})
	id := sop.NewUUID()
	node := &btree.Node[PersonKey, Person]{ID: id, Version: 1}
	h := sop.NewHandle(id)
	h.Version = 1
	reg.Lookup[id] = h
	nr := &nodeRepositoryBackend{transaction: tx, storeInfo: si, l2Cache: l2, l1Cache: cache.GetGlobalCache(), localCache: map[sop.UUID]cachedNode{id: {action: updateAction, node: node}}, count: si.Count}
	tx.btreesBackend = []btreeBackend{{
		nodeRepository:                   nr,
		getStoreInfo:                     func() *sop.StoreInfo { return si },
		hasTrackedItems:                  func() bool { return true },
		checkTrackedItems:                func(context.Context) error { return nil },
		lockTrackedItems:                 func(context.Context, time.Duration) error { return nil },
		unlockTrackedItems:               func(context.Context) error { return nil },
		commitTrackedItemsValues:         func(context.Context) error { return nil },
		getForRollbackTrackedItemsValues: func() *sop.BlobsPayload[sop.UUID] { return nil },
		getObsoleteTrackedItemsValues:    func() *sop.BlobsPayload[sop.UUID] { return nil },
		refetchAndMerge:                  func(context.Context) error { return nil },
	}}
	if err := tx.phase1Commit(ctx); err == nil || err.Error() != "log add err nth" {
		t.Fatalf("expected log error at refetch lockTrackedItems, got: %v", err)
	}
}

// No mutations: hasTrackedItems true but with no added/updated/removed/fetched/root nodes; phase1Commit should succeed fast.
func Test_Phase1Commit_NoMutations_Succeeds(t *testing.T) {
	ctx := context.Background()
	l2 := mocks.NewMockClient()
	cache.NewGlobalCache(l2, cache.DefaultMinCapacity, cache.DefaultMaxCapacity)
	bs := mocks.NewMockBlobStore()
	sr := mocks.NewMockStoreRepository()
	tl := newTransactionLogger(stubTLog{pl: &stubPriorityLog{}}, true)
	tx := &Transaction{mode: sop.ForWriting, maxTime: time.Minute, StoreRepository: sr, registry: mocks.NewMockRegistry(false), l2Cache: l2, l1Cache: cache.GetGlobalCache(), blobStore: bs, logger: tl, phaseDone: 0}
	si := sop.NewStoreInfo(sop.StoreOptions{Name: "p1_no_mut", SlotLength: 2})
	nr := &nodeRepositoryBackend{transaction: tx, storeInfo: si, l2Cache: l2, l1Cache: cache.GetGlobalCache(), localCache: map[sop.UUID]cachedNode{}, count: si.Count}
	tx.btreesBackend = []btreeBackend{{
		nodeRepository:                   nr,
		getStoreInfo:                     func() *sop.StoreInfo { return si },
		hasTrackedItems:                  func() bool { return true },
		checkTrackedItems:                func(context.Context) error { return nil },
		lockTrackedItems:                 func(context.Context, time.Duration) error { return nil },
		unlockTrackedItems:               func(context.Context) error { return nil },
		commitTrackedItemsValues:         func(context.Context) error { return nil },
		getForRollbackTrackedItemsValues: func() *sop.BlobsPayload[sop.UUID] { return nil },
		getObsoleteTrackedItemsValues:    func() *sop.BlobsPayload[sop.UUID] { return nil },
		refetchAndMerge:                  func(context.Context) error { return nil },
	}}
	if err := tx.phase1Commit(ctx); err != nil {
		t.Fatalf("phase1Commit with no mutations err: %v", err)
	}
}

// Multi-store: verify commitStores/rollbackStores info aggregation across more than one backend.
func Test_Phase1Commit_MultiStore_CommitStoresInfo_Aggregates(t *testing.T) {
	ctx := context.Background()
	l2 := mocks.NewMockClient()
	cache.NewGlobalCache(l2, cache.DefaultMinCapacity, cache.DefaultMaxCapacity)
	bs := mocks.NewMockBlobStore()
	baseSR := mocks.NewMockStoreRepository()
	sr := baseSR
	tl := newTransactionLogger(stubTLog{pl: &stubPriorityLog{}}, true)
	reg := mocks.NewMockRegistry(false)
	tx := &Transaction{mode: sop.ForWriting, maxTime: time.Minute, StoreRepository: sr, registry: reg, l2Cache: l2, l1Cache: cache.GetGlobalCache(), blobStore: bs, logger: tl, phaseDone: 0}

	si1 := sop.NewStoreInfo(sop.StoreOptions{Name: "p1_mstore_1", SlotLength: 2})
	si2 := sop.NewStoreInfo(sop.StoreOptions{Name: "p1_mstore_2", SlotLength: 2})
	nr1 := &nodeRepositoryBackend{transaction: tx, storeInfo: si1, l2Cache: l2, l1Cache: cache.GetGlobalCache(), localCache: map[sop.UUID]cachedNode{}, count: si1.Count}
	nr2 := &nodeRepositoryBackend{transaction: tx, storeInfo: si2, l2Cache: l2, l1Cache: cache.GetGlobalCache(), localCache: map[sop.UUID]cachedNode{}, count: si2.Count}
	tx.btreesBackend = []btreeBackend{
		{
			nodeRepository:                   nr1,
			getStoreInfo:                     func() *sop.StoreInfo { return si1 },
			hasTrackedItems:                  func() bool { return true },
			checkTrackedItems:                func(context.Context) error { return nil },
			lockTrackedItems:                 func(context.Context, time.Duration) error { return nil },
			unlockTrackedItems:               func(context.Context) error { return nil },
			commitTrackedItemsValues:         func(context.Context) error { return nil },
			getForRollbackTrackedItemsValues: func() *sop.BlobsPayload[sop.UUID] { return nil },
			getObsoleteTrackedItemsValues:    func() *sop.BlobsPayload[sop.UUID] { return nil },
			refetchAndMerge:                  func(context.Context) error { return nil },
		},
		{
			nodeRepository:                   nr2,
			getStoreInfo:                     func() *sop.StoreInfo { return si2 },
			hasTrackedItems:                  func() bool { return true },
			checkTrackedItems:                func(context.Context) error { return nil },
			lockTrackedItems:                 func(context.Context, time.Duration) error { return nil },
			unlockTrackedItems:               func(context.Context) error { return nil },
			commitTrackedItemsValues:         func(context.Context) error { return nil },
			getForRollbackTrackedItemsValues: func() *sop.BlobsPayload[sop.UUID] { return nil },
			getObsoleteTrackedItemsValues:    func() *sop.BlobsPayload[sop.UUID] { return nil },
			refetchAndMerge:                  func(context.Context) error { return nil },
		},
	}

	if err := tx.phase1Commit(ctx); err != nil {
		t.Fatalf("phase1Commit err: %v", err)
	}

	// Ensure commitStores info can be produced for multiple backends without panic and deltas computed.
	cs := tx.getCommitStoresInfo()
	if len(cs) != 2 {
		t.Fatalf("expected 2 store infos, got %d", len(cs))
	}
	rb := tx.getRollbackStoresInfo()
	if len(rb) != 2 {
		t.Fatalf("expected 2 rollback store infos, got %d", len(rb))
	}
}
